{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting NYC Taxi Fares with RAPIDS\n",
    "\n",
    "[RAPIDS](https://rapids.ai/) is a suite of GPU accelerated data science libraries with APIs that should be familiar to users of Pandas, Dask, and Scikitlearn.\n",
    "\n",
    "Anaconda has graciously made some of the NYC Taxi dataset available in [a public Google Cloud Storage bucket](https://console.cloud.google.com/storage/browser/anaconda-public-data/nyc-taxi/csv/). We have already cleaned up the column names a bit and re-saved a single month of the data. (For a larger-scale exercise, consider adapting this script to handle a full year via Dask!) We saved the converted version in an S3 bucket here: https://odsc-sample-data.s3-us-west-2.amazonaws.com/yellow_tripdata_2014-03-cleaned.orc.\n",
    "\n",
    "This notebook builds a simple data pipeline to load the data with cuDF (or Pandas), analyze it with cuML (or scikit-learn), and then try some steps with multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cuml\n",
    "import cudf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1: Inspecting the Data\n",
    "\n",
    "Let's start with a familiar Pandas approach then port it to RAPIDS in parallel. Note that both Pandas and cuDF can read directly from an S3 bucket. If you're going to re-run this example many times or run outside of AWS, consider saving the file locally (it's about 400MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Pandas\n",
    "\n",
    "df = pd.read_orc('yellow_tripdata_2014-03-cleaned.orc') # Alternative for local read\n",
    "# df = pd.read_orc('https://odsc-sample-data.s3-us-west-2.amazonaws.com/yellow_tripdata_2014-03-cleaned.orc')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TODO: Read the CSV with cudf into 'gdf' and display the first few rows\n",
    "\n",
    "# *** Answer: ***\n",
    "\n",
    "gdf = cudf.read_orc('yellow_tripdata_2014-03-cleaned.orc')\n",
    "# gdf = cudf.read_orc('https://odsc-sample-data.s3-us-west-2.amazonaws.com/yellow_tripdata_2014-03-cleaned.orc')\n",
    "\n",
    "\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at some key stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When passing data to functions that expect Pandas DataFrames, we just use \".to_pandas()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(x=\"passenger_count\", y=\"fare_amount\", data=gdf.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"pickup_longitude\", y=\"pickup_latitude\",\n",
    "           data=gdf.head(100000).to_pandas(),\n",
    "           fit_reg=False,\n",
    "           x_jitter=0.01, y_jitter=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the data\n",
    "\n",
    "Ok, there are some WEIRD latitudes and longitudes in that data. Let's filter to just sane stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# apply a list of filter conditions to throw out records with missing or outlier values\n",
    "query_frags = [\n",
    "    'fare_amount > 0 and fare_amount < 500',\n",
    "    'passenger_count > 0 and passenger_count < 6',\n",
    "    'pickup_longitude > -75.0 and pickup_longitude < -73.0',\n",
    "    'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "    'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "    'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "]\n",
    "df_subset = df.query(' and '.join(query_frags)).copy()\n",
    "\n",
    "# inspect the results of cleaning\n",
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: RAPIDS version with \"gdf_subset\" as output\n",
    "\n",
    "# *** Answer:\n",
    "gdf_subset = gdf.query(' and '.join(query_frags)).copy()\n",
    "gdf_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 2: UDFs to add rich features (more advanced topic)\n",
    "\n",
    "cuDF provides standard DataFrame operations, but also let you run \"user defined functions\" on the underlying data. For simple operations on a single column, you can just pass in a standard Python function or lambda.\n",
    "\n",
    "cuDF's [apply_rows](https://docs.rapids.ai/api/cudf/stable/guide-to-udfs.html#DataFrame-UDFs) operation is similar to Pandas's [DataFrame.apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html), except that for cuDF, custom Python code is [JIT compiled by numba](https://numba.pydata.org/numba-doc/dev/cuda/kernels.html) into GPU kernels. This allows you to port complex functions that use multiple columns from the DataFrame.\n",
    "\n",
    "We'll use a Haversine Distance calculation to find total trip distance, and extract additional useful variables from the datetime fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import pi\n",
    "\n",
    "def haversine_distance_kernel_cpu(row):\n",
    "    x_1, y_1, x_2, y_2 = (row[\"pickup_latitude\"], row[\"pickup_longitude\"], row[\"dropoff_latitude\"], row[\"dropoff_longitude\"])\n",
    "    x_1 = pi/180 * x_1\n",
    "    y_1 = pi/180 * y_1\n",
    "    x_2 = pi/180 * x_2\n",
    "    y_2 = pi/180 * y_2\n",
    "\n",
    "    dlon = y_2 - y_1\n",
    "    dlat = x_2 - x_1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(x_1) * np.cos(x_2) * np.sin(dlon/2)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers\n",
    "        \n",
    "    return c * r\n",
    "        \n",
    "def add_features(df):\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day'] = df['pickup_datetime'].dt.day\n",
    "    \n",
    "    df['pickup_latitude_r'] = df['pickup_latitude']//.01*.01\n",
    "    df['pickup_longitude_r'] = df['pickup_longitude']//.01*.01\n",
    "    df['dropoff_latitude_r'] = df['dropoff_latitude']//.01*.01\n",
    "    df['dropoff_longitude_r'] = df['dropoff_longitude']//.01*.01\n",
    "    df[\"day_of_week\"] = df[\"pickup_datetime\"].dt.dayofweek\n",
    "    \n",
    "    df = df.drop(columns=['pickup_datetime', 'dropoff_datetime'])    \n",
    "    \n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(np.int32)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# actually add the features\n",
    "taxi_df = add_features(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# compute distance\n",
    "taxi_df[\"h_distance\"] = haversine_distance_kernel_cpu(taxi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cuDF version with UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cuDF's [apply_rows](https://rapidsai.github.io/projects/cudf/en/0.6.0/api.html#cudf.dataframe.DataFrame.apply_rows) operation is similar to Pandas's [DataFrame.apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html), except that for cuDF, custom Python code is [JIT compiled by numba](https://numba.pydata.org/numba-doc/dev/cuda/kernels.html) into GPU kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "\n",
    "def haversine_distance_kernel_gpu(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude, h_distance):\n",
    "    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude)):\n",
    "        x_1 = pi/180 * x_1\n",
    "        y_1 = pi/180 * y_1\n",
    "        x_2 = pi/180 * x_2\n",
    "        y_2 = pi/180 * y_2\n",
    "        \n",
    "        dlon = y_2 - y_1\n",
    "        dlat = x_2 - x_1\n",
    "        a = sin(dlat/2)**2 + cos(x_1) * cos(x_2) * sin(dlon/2)**2\n",
    "        \n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        r = 6371 # Radius of earth in kilometers\n",
    "        \n",
    "        h_distance[i] = c * r\n",
    "  \n",
    "def add_features_gpu(df):\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day'] = df['pickup_datetime'].dt.day\n",
    "    \n",
    "    df['pickup_latitude_r'] = df['pickup_latitude']//.01*.01\n",
    "    df['pickup_longitude_r'] = df['pickup_longitude']//.01*.01\n",
    "    df['dropoff_latitude_r'] = df['dropoff_latitude']//.01*.01\n",
    "    df['dropoff_longitude_r'] = df['dropoff_longitude']//.01*.01\n",
    "    df[\"day_of_week\"] = df[\"pickup_datetime\"].dt.dayofweek\n",
    "    \n",
    "    df = df.drop(columns=['pickup_datetime', 'dropoff_datetime'])    \n",
    " \n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(np.int32)\n",
    "    return df\n",
    "\n",
    "def compute_distance_gpu(df):\n",
    "    df = df.apply_rows(haversine_distance_kernel_gpu,\n",
    "                   incols=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'],\n",
    "                   outcols=dict(h_distance=np.float32),\n",
    "                   kwargs=dict())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: actually add the features and create \"taxi_gdf\" from gdf_subset\n",
    "\n",
    "# Answer\n",
    "taxi_gdf = add_features_gpu(gdf_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: add the distance calculation\n",
    "\n",
    "# *** Answer\n",
    "taxi_gdf = compute_distance_gpu(taxi_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more advanced spatial calculations, check out cuSpatial (https://medium.com/rapids-ai/releasing-cuspatial-to-accelerate-geospatial-and-spatiotemporal-processing-b686d8b32a9), the newest RAPIDS library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: Print summary stats from \"taxi_gdf\"\n",
    "\n",
    "# *** Answer:\n",
    "taxi_gdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 4: Cluster and analyze with cuML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn, sklearn.cluster\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# use scikit-learn on CPU\n",
    "n_samples = len(taxi_df)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sk_kmeans = sklearn.cluster.KMeans(n_clusters=5)\n",
    "scaler = StandardScaler()\n",
    "taxi_subset = taxi_df.iloc[:n_samples]\n",
    "#train_clusters_cpu = sk_kmeans.fit_predict(\n",
    "#    scaler.fit_transform(taxi_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TODO: use cuML on GPU to fit KMeans with 5 clusters (larger dataset)\n",
    "# Note that preprocessing is an experimental module for cuML in 0.16\n",
    "\n",
    "# *** Answer:\n",
    "import cuml.cluster\n",
    "from cuml.experimental.preprocessing import StandardScaler\n",
    "cu_kmeans = cuml.cluster.KMeans(n_clusters=5)\n",
    "scaler = StandardScaler()\n",
    "train_clusters_gpu = cu_kmeans.fit_predict(\n",
    "    scaler.fit_transform(taxi_gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just take a subset to speed plotting\n",
    "gdf_head = taxi_gdf.iloc[:400000]\n",
    "gdf_head[\"cluster\"] = train_clusters_gpu[:400000]\n",
    "gdf_head[\"short_trip\"] = gdf_head[\"trip_distance\"] < 1.01 # About the 25th percentile\n",
    "gdf_head[\"is_rush_est\"] = ((gdf_head.hour >= 10) & (gdf_head.hour <= 14)) | \\\n",
    "                                ((gdf_head.hour >= 21) & (gdf_head.hour <= 24))\n",
    "\n",
    "# actually do the plot\n",
    "sns.lmplot(\"pickup_longitude\", \"pickup_latitude\", data=gdf_head.to_pandas(),\n",
    "           hue=\"cluster\", col=\"is_rush_est\", row=\"short_trip\", fit_reg=False, scatter_kws={\"s\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick a Training Set\n",
    "\n",
    "Let's imagine we want to be able predict fare prices based on the available data. We'll start by splitting the data into train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split as sk_train_test_split\n",
    "\n",
    "X_np = taxi_df.drop(columns='fare_amount').to_numpy()\n",
    "Y_np = taxi_df[[\"fare_amount\"]].to_numpy()\n",
    "\n",
    "X_train_np, X_test_np, Y_train_np, Y_test_np = sk_train_test_split(X_np, Y_np, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from cuml.preprocessing.model_selection import train_test_split as cu_train_test_split\n",
    "\n",
    "X_gpu = taxi_gdf.drop(columns='fare_amount').values.astype(\"float32\")\n",
    "Y_gpu = taxi_gdf[[\"fare_amount\"]].values.astype(\"float32\")\n",
    "\n",
    "X_col_names = taxi_gdf.columns.tolist()\n",
    "X_col_names.remove('fare_amount')\n",
    "\n",
    "X_train_gpu, X_test_gpu, Y_train_gpu, Y_test_gpu = cu_train_test_split(X_gpu,\n",
    "                                                                        Y_gpu,\n",
    "                                                                        test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a simple supervised model with cuML\n",
    "\n",
    "cuML supports a large range of supervised models, all emulating the scikit-learn interfaces. See the README (https://github.com/rapidsai/cuml) for a recent list. Here, we'll try a very simple model - an ElasticNet regularized regression with both L1 and L2 regularization. As a user exercise, try replacing this with a RandomForestRegressor or a simpler LinearRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet as skElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Sklearn will parallelize over all CPU cores with n_jobs=-1\n",
    "sk_model = skElasticNet()\n",
    "sk_model.fit(X_train_np, Y_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.regression import r2_score as sk_r2_score\n",
    "\n",
    "print(\"Out-of-sample (test) R2: \", sk_r2_score(Y_test_np, sk_model.predict(X_test_np)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: Build a similar model on GPU with cuML\n",
    "\n",
    "# *** Answer:\n",
    "from cuml.linear_model import ElasticNet as cuElasticNet\n",
    "cu_model = cuElasticNet(alpha=0.1)\n",
    "cu_model.fit(X_train_gpu, Y_train_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: Predict on the test set and evaluate the predictions' R2 score\n",
    "\n",
    "# *** Answer\n",
    "from cuml.metrics.regression import r2_score\n",
    "\n",
    "Y_hat_gpu = cu_model.predict(X_test_gpu)\n",
    "r2_score(Y_test_gpu, Y_hat_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 5: Train an  XGBoost Regression Model\n",
    "\n",
    "XGBoost is one of the most popular packages for gradient boosted decision trees. It comes with excellent GPU acceleration out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train on CPU (uses all CPUs by default)\n",
    "import xgboost\n",
    "\n",
    "params = {\n",
    " 'learning_rate': 0.3,\n",
    "  'max_depth': 6,\n",
    "\n",
    "  'subsample': 0.6,\n",
    "  'gamma': 1\n",
    "}\n",
    "\n",
    "train_dmat = xgboost.DMatrix(X_train_np, Y_train_np, feature_names=X_col_names)\n",
    "print(\"Converted to dmatrix\")\n",
    "trained_model = xgboost.train(params, train_dmat, num_boost_round=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: retrain on GPU for more rounds, saving model as trained_model_gpus\n",
    "\n",
    "# *** Answer:\n",
    "params = {\n",
    " 'learning_rate': 0.3,\n",
    "  'max_depth': 6,\n",
    "  'tree_method': 'gpu_hist',\n",
    "  'subsample': 0.6,\n",
    "  'gamma': 1\n",
    "}\n",
    "\n",
    "train_dmat = xgboost.DMatrix(X_train_gpu, Y_train_gpu, feature_names=X_col_names)\n",
    "print(\"Converted to dmatrix\")\n",
    "trained_model_gpu = xgboost.train(params, train_dmat, num_boost_round=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Good is Our Model?\n",
    "\n",
    "Now that we have a trained model, we need to test it with the ecords we held out.Y_train_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: generate predictions on the test set as Y_test_prediction\n",
    "\n",
    "# *** Answer:\n",
    "# Note that we can pass in a cuDF dataframe without conversion\n",
    "test_dmat = xgboost.DMatrix(X_test_np, feature_names=X_col_names)\n",
    "\n",
    "Y_test_prediction = trained_model_gpu.predict(test_dmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Trained Model for Later Use\n",
    "\n",
    "To make a model maximally useful, you need to be able to save it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trained_model_gpu.save_model(\"output.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
